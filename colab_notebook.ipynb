{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyGxedwgnDuf"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install sentencepiece #for T5\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "def get_label_categories(columns, output_column):\n",
        "    client = OpenAI(api_key=os.environ[\"openai_key\"])\n",
        "\n",
        "    response_message = []\n",
        "\n",
        "    while len(response_message) != 3:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": f\"We need to solve a classifcation problem, \"\n",
        "                                            f\"We have a database with the following columns {', '.join(columns)}, where {output_column} is the outcome column we want to predict.\"\n",
        "                                            \"Please generate carefully the best question and labels to use on the column we want to predict .\\n\"\n",
        "                                            \"IMPORTANT: Give only the question and the labels seperated by comma\\n\"\n",
        "                                            \"use this format: \\n\"\n",
        "                                            \"Question, positive_label, negative_label\"\n",
        "                 },\n",
        "            ]\n",
        "\n",
        "        )\n",
        "        response_message = response.choices[0].message.content.replace('\\n', '').split(',')\n",
        "\n",
        "    return response_message[0], response_message[1], response_message[2]"
      ],
      "metadata": {
        "id": "W2wxNyx5o17Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def get_string_data(data_frame,question, label_column_name):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for index, row in data_frame.iterrows():\n",
        "        # Construct the formatted string for the current row\n",
        "        row_string = ', '.join([f'{column}: {value}' for column, value in row.items() if column != label_column_name])\n",
        "        fullMsg = f\"{row_string}. {question}\"\n",
        "\n",
        "        texts.append(row_string)\n",
        "        labelInt = int(row[label_column_name])\n",
        "        labelText = positiveLabel if labelInt == 1 else negativeLabel\n",
        "        labels.append(labelText)\n",
        "\n",
        "    data_set = {\n",
        "        'texts': texts,\n",
        "        'labels': labels,\n",
        "    }\n",
        "\n",
        "    return data_set\n"
      ],
      "metadata": {
        "id": "rjSWAQun0LgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "from transformers import TrainingArguments, AutoTokenizer, Trainer, DistilBertForSequenceClassification\n",
        "from transformers import TrainerCallback, EarlyStoppingCallback\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "\n",
        "def processDataSet(datasetPath,question, label_column_name):\n",
        "  data = pd.read_csv(datasetPath)\n",
        "  if len(data) > 5000:\n",
        "      data = data.head(5000)\n",
        "  train_data = data.sample(frac=0.8, random_state=25)  # 80% for training\n",
        "  test_data = data.drop(train_data.index)   # 20% for testing\n",
        "\n",
        "  dataset = DatasetDict(\n",
        "      train = Dataset.from_dict(get_string_data(train_data, question, label_column_name)),\n",
        "      test = Dataset.from_dict(get_string_data(test_data, question, label_column_name)),\n",
        "  )\n",
        "\n",
        "  def preprocess_data(examples):\n",
        "    model_inputs = tokenizer(examples['texts'], max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "      labels = tokenizer(examples[\"labels\"], max_length=max_target_length,\n",
        "                        truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "  tokenized_datasets = dataset.map(preprocess_data, batched=True)\n",
        "  return tokenized_datasets, test_data"
      ],
      "metadata": {
        "id": "Lr_J72CznCDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, AutoTokenizer, Trainer, DistilBertForSequenceClassification\n",
        "from transformers import TrainerCallback, EarlyStoppingCallback, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
        "import sentencepiece\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def get_trainer(tokenized_datasets):\n",
        "  data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
        "\n",
        "  arguments = Seq2SeqTrainingArguments(\n",
        "      output_dir=\"sample_hf_trainer\",\n",
        "      per_device_train_batch_size=16,\n",
        "      per_device_eval_batch_size=16,\n",
        "      num_train_epochs=20,\n",
        "      # evaluation_strategy=\"epoch\",  # run validation at the end of each epoch\n",
        "      save_strategy='no',\n",
        "      do_eval=False,\n",
        "      evaluation_strategy=\"no\",\n",
        "      learning_rate=2e-5,\n",
        "      # load_best_model_at_end=True,\n",
        "      seed=224,\n",
        "  )\n",
        "\n",
        "  def compute_metrics(eval_pred):\n",
        "      # for T5\n",
        "      \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
        "      predictions, labels = eval_pred\n",
        "      # Decode the predictions\n",
        "      predictions = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n",
        "      # Calculate the accuracy\n",
        "      return {\"accuracy\": np.mean([pred == label for pred, label in zip(predictions, labels)])}\n",
        "\n",
        "\n",
        "  trainer = Seq2SeqTrainer(\n",
        "      model=model,\n",
        "      args=arguments,\n",
        "      train_dataset=tokenized_datasets['train'],\n",
        "      # eval_dataset=small_tokenized_dataset['val'], # change to test when you do your final evaluation!\n",
        "      # eval_dataset=eval_dataset,  # change to test when you do your final evaluation!\n",
        "      tokenizer=tokenizer,\n",
        "      compute_metrics=compute_metrics,\n",
        "      data_collator=data_collator\n",
        "  )\n",
        "\n",
        "\n",
        "  class LoggingCallback(TrainerCallback):\n",
        "      def __init__(self, log_path):\n",
        "          self.log_path = log_path\n",
        "\n",
        "      def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "          _ = logs.pop(\"total_flos\", None)\n",
        "          if state.is_local_process_zero:\n",
        "              with open(self.log_path, \"a\") as f:\n",
        "                  f.write(json.dumps(logs) + \"\\n\")\n",
        "\n",
        "\n",
        "  trainer.add_callback(LoggingCallback(\"sample_hf_trainer/log.jsonl\"))\n",
        "  return trainer"
      ],
      "metadata": {
        "id": "m5fsH3TZybzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import softmax\n",
        "\n",
        "\n",
        "def run_test_data(test_data, question, label_column_name):\n",
        "  test_data_set = get_string_data(test_data, question, label_column_name)\n",
        "  correct = 0\n",
        "  true_positive = 0\n",
        "  false_positive = 0\n",
        "  true_negative = 0\n",
        "  false_negative = 0\n",
        "  for i, text in enumerate(test_data_set['texts']):\n",
        "    label = test_data_set['labels'][i]\n",
        "    inputs = tokenizer([text], max_length=max_input_length, truncation=True, return_tensors=\"pt\").to('cuda')\n",
        "    output = model.generate(**inputs, num_beams=16, do_sample=True, min_length=0, max_length=64)\n",
        "    label_string = label.strip()\n",
        "    labelInt = 1 if label_string == positiveLabel else 0\n",
        "    y_true.append(labelInt)\n",
        "    decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "    output_string = decoded_output.strip()\n",
        "    outputInt = 1 if output_string == positiveLabel else 0\n",
        "    y_score.append(outputInt)\n",
        "    if output_string == label_string:\n",
        "      correct += 1\n",
        "      if label_string == positiveLabel:\n",
        "        true_positive += 1\n",
        "      else:\n",
        "        true_negative += 1\n",
        "    else:\n",
        "      if output_string == positiveLabel:\n",
        "        false_positive += 1\n",
        "      else:\n",
        "        false_negative += 1\n",
        "  length = len(test_data_set['texts'])\n",
        "  print(f\"predicted {correct} out of {length}. percentage {((correct / length) * 100)}\")"
      ],
      "metadata": {
        "id": "uNOrb7ZDSKOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calculate_statistics():\n",
        "  precision = precision_score(y_true, y_score)\n",
        "  recall = recall_score(y_true, y_score)\n",
        "  f1 = f1_score(y_true, y_score)\n",
        "\n",
        "  print(f'Precision: {precision}')\n",
        "  print(f'Recall: {recall}')\n",
        "  print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "id": "Zev-_S7oc2sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "def get_data_set(info):\n",
        "  owner = info[\"owner\"]\n",
        "  dataset = info[\"dataset\"]\n",
        "  csvFileName = info[\"csvFileName\"]\n",
        "\n",
        "  datasetFullName = f\"{owner}/{dataset}\"\n",
        "  mainFolder = \"datasets\"\n",
        "\n",
        "  if owner == \"competitions\":\n",
        "    !kaggle competitions download -c {dataset}\n",
        "    mainFolder = \".\"\n",
        "  else:\n",
        "    !kaggle datasets download -d {datasetFullName}\n",
        "\n",
        "\n",
        "  datasetFolder = f\"/content/kaggle/{mainFolder}/{owner}/{dataset}\"\n",
        "  csv_file_path = f\"{datasetFolder}/{csvFileName}\"\n",
        "\n",
        "  with zipfile.ZipFile(f\"{datasetFolder}/{dataset}.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extract(f\"{csvFileName}\", datasetFolder)  # Extract to a specific directory\n",
        "  return csv_file_path\n"
      ],
      "metadata": {
        "id": "ISpC_e480oCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_dataset_params(datasetInfo):\n",
        "  question = datasetInfo[\"question\"]\n",
        "  inputSuffix = f\"\\nPlease answer the following question.\\n{question} {positiveLabel} or {negativeLabel}?\"\n",
        "  maxLength = max(len(positiveLabel), len(negativeLabel))\n",
        "  return inputSuffix, maxLength\n"
      ],
      "metadata": {
        "id": "WYK2wh2X40YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(dataset, method):\n",
        "  assert method in [\"true-false\", \"yes-no\", \"llm-based\"]\n",
        "\n",
        "  if method == \"true-false\":\n",
        "    dataset[\"positiveLabel\"] = \"true\"\n",
        "    dataset[\"negativeLabel\"] = \"false\"\n",
        "  elif method == \"yes-no\":\n",
        "    dataset[\"positiveLabel\"] = \"yes\"\n",
        "    dataset[\"negativeLabel\"] = \"no\"\n",
        "  elif method == \"llm-based\":\n",
        "    question, pos_label, neg_label = get_label_categories(dataset)\n",
        "    dataset[\"question\"] = question\n",
        "    dataset[\"positiveLabel\"] = pos_label\n",
        "    dataset[\"negativeLabel\"] = neg_label"
      ],
      "metadata": {
        "id": "CxOmkE6XOLRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# upload kaggle.json with credentials to /content\n",
        "\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content\"\n",
        "\n",
        "!kaggle config set -n path -v \"/content/kaggle\"\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "max_input_length = 512\n",
        "max_target_length = 64\n",
        "\n",
        "datasets= [\n",
        "    { # 0 means healthy\n",
        "      \"owner\": \"uciml\",\n",
        "      \"dataset\": \"pima-indians-diabetes-database\",\n",
        "      \"csvFileName\": \"diabetes.csv\",\n",
        "      \"labelColumnName\": \"Outcome\",\n",
        "      \"question\": \"Does the patient have diabetes?\",\n",
        "      \"positiveLabel\": \"Sick\",\n",
        "      \"negativeLabel\": \"Healthy\",\n",
        "    },\n",
        "    { # 1 means survived\n",
        "      \"owner\": \"competitions\",\n",
        "      \"dataset\": \"titanic\",\n",
        "      \"csvFileName\": \"train.csv\",\n",
        "      \"labelColumnName\": \"Survived\",\n",
        "      \"question\": \"Did the person survive?\",\n",
        "      \"positiveLabel\": \"Alive\",\n",
        "      \"negativeLabel\": \"Dead\",\n",
        "    },\n",
        "    { # 1 means heart disease\n",
        "      \"owner\": \"fedesoriano\",\n",
        "      \"dataset\": \"heart-failure-prediction\",\n",
        "      \"csvFileName\": \"heart.csv\",\n",
        "      \"labelColumnName\": \"HeartDisease\",\n",
        "      \"question\": \"Does the person have heart disease?\",\n",
        "      \"positiveLabel\": \"Sick\",\n",
        "      \"negativeLabel\": \"Healthy\",\n",
        "    },\n",
        "    { # 1 means heart disease\n",
        "      \"owner\": \"mastmustu\",\n",
        "      \"dataset\": \"income\",\n",
        "      \"csvFileName\": \"train.csv\",\n",
        "      \"labelColumnName\": \"income_>50K\",\n",
        "      \"question\": \"Is the person income greater than 50K?\",\n",
        "      \"positiveLabel\": \"Rich\",\n",
        "      \"negativeLabel\": \"Poor\",\n",
        "    },\n",
        "]\n",
        "\n",
        "for dataset in datasets:\n",
        "  print(f\"running on dataset {dataset['dataset']}\")\n",
        "\n",
        "  y_true = [] # true labels\n",
        "  y_score = [] # predicted scores\n",
        "  positiveLabel = dataset[\"positiveLabel\"] if dataset.get(\"positiveLabel\") else \"Yes\"\n",
        "  negativeLabel = dataset[\"negativeLabel\"] if dataset.get(\"negativeLabel\") else \"No\"\n",
        "  test_data = None\n",
        "  csv_file_path = get_data_set(dataset)\n",
        "  question, maxLength = set_dataset_params(dataset)\n",
        "  tokenized_datasets, test_data = processDataSet(csv_file_path, question, dataset['labelColumnName'])\n",
        "  trainer = get_trainer(tokenized_datasets)\n",
        "  trainer.train()\n",
        "  trainer.save_model()\n",
        "  run_test_data(test_data, question, dataset['labelColumnName'])\n",
        "  calculate_statistics()"
      ],
      "metadata": {
        "id": "_D01hupruk2V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}